{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CIFAR-10 Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import cifar10_lib\n",
    "from cifar10_lib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)\n",
    "\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build conv_net conv_plan[3] conv_depth[3] full_plan[5]\n",
      "Add conv2d_maxpool: ReLU[1] dropout[1] conv kernel[4x4] stride[2x2] pool kernal[2x2] stride[2x2] output[32]\n",
      "  conv2d_maxpool input:  [None, 32, 32, 3] \tsize = 3072\n",
      "  conv2d_maxpool cov2d:  [None, 16, 16, 32] \tsize = 8192\n",
      "  conv2d_maxpool add dropout\n",
      "  conv2d_maxpool  pool:  [None, 8, 8, 32] \tsize = 2048\n",
      "Add conv2d_maxpool: ReLU[1] dropout[1] conv kernel[4x4] stride[2x2] pool kernal[2x2] stride[2x2] output[128]\n",
      "  conv2d_maxpool input:  [None, 8, 8, 32] \tsize = 2048\n",
      "  conv2d_maxpool cov2d:  [None, 4, 4, 128] \tsize = 2048\n",
      "  conv2d_maxpool add dropout\n",
      "  conv2d_maxpool  pool:  [None, 2, 2, 128] \tsize = 512\n",
      "Add conv2d_maxpool: ReLU[1] dropout[1] conv kernel[4x4] stride[2x2] pool kernal[2x2] stride[2x2] output[512]\n",
      "  conv2d_maxpool input:  [None, 2, 2, 128] \tsize = 512\n",
      "  conv2d_maxpool cov2d:  [None, 1, 1, 512] \tsize = 512\n",
      "  conv2d_maxpool add dropout\n",
      "  conv2d_maxpool  pool:  [None, 1, 1, 512] \tsize = 512\n",
      "Add flatten: size[512]\n",
      "Add fully_conn: input[512] output[1024]\n",
      "Add dropout layer\n",
      "Add fully_conn: input[1024] output[256]\n",
      "Add fully_conn: input[256] output[64]\n",
      "Add output: input[64] output[10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides, keep_prob=None, dropout=False, relu=True):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # ---- convolution layer ----\n",
    "    print(\"Add conv2d_maxpool: ReLU[%d] dropout[%d] conv kernel[%dx%d] stride[%dx%d] pool kernal[%dx%d] stride[%dx%d] output[%d]\" % \\\n",
    "           (relu, dropout, \\\n",
    "            conv_ksize[0],conv_ksize[1],conv_strides[0],conv_strides[1],\\\n",
    "            pool_ksize[0], pool_ksize[1],pool_strides[0], pool_strides[1],conv_num_outputs))\n",
    "    conv_input_size = getTensorSize(x_tensor)\n",
    "    conv_output_size = (x_tensor.get_shape().as_list()[1] / conv_strides[0]) ** 2 * conv_num_outputs\n",
    "    print(\"  conv2d_maxpool input: \",x_tensor.get_shape().as_list(),\"\\tsize =\",conv_input_size)\n",
    "    n_channels = x_tensor.get_shape().as_list()[3]\n",
    "    \n",
    "    shape_W = [conv_ksize[0],conv_ksize[1],n_channels,conv_num_outputs]\n",
    "    shape_b = [conv_num_outputs]\n",
    "    # F_W = tf.Variable(tf.random_normal(shape_W))\n",
    "    # F_b = tf.Variable(tf.random_normal(shape_b))\n",
    "    \n",
    "    F_W = tf.Variable(xavier_init(shape_W, conv_input_size, conv_output_size))\n",
    "    F_b = tf.Variable(xavier_init(shape_b, conv_input_size, conv_output_size))\n",
    "    #F_W = tf.get_variable(shape=shape_W, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    #F_b = tf.get_variable(shape=shape_b, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    x_tensor = tf.nn.conv2d(x_tensor, F_W, [1,conv_strides[0],conv_strides[1],1], 'SAME') + F_b\n",
    "    if relu:\n",
    "        x_tensor = tf.nn.relu(x_tensor)\n",
    "    print(\"  conv2d_maxpool cov2d: \",x_tensor.get_shape().as_list(),\"\\tsize =\",getTensorSize(x_tensor))\n",
    "    if dropout:\n",
    "        print(\"  conv2d_maxpool add dropout\")\n",
    "        x_tensor = tf.nn.dropout(x_tensor, keep_prob)\n",
    "    \n",
    "    # ---- max pooling layer ----\n",
    "    x_tensor = tf.nn.max_pool(x_tensor, ksize=[1, pool_ksize[0], pool_ksize[1], 1],\\\n",
    "                              strides=[1, pool_strides[0], pool_strides[1], 1], padding='SAME')\n",
    "    print(\"  conv2d_maxpool  pool: \",x_tensor.get_shape().as_list(),\"\\tsize =\",getTensorSize(x_tensor))\n",
    "    return x_tensor \n",
    "\n",
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    flat_size = getTensorSize(x_tensor)\n",
    "    print(\"Add flatten: size[%d]\" % (flat_size))\n",
    "    return tf.reshape(x_tensor, [-1, flat_size])\n",
    "\n",
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_size = x_tensor.get_shape().as_list()[1]\n",
    "    print(\"Add fully_conn: input[%d] output[%d]\" % (input_size,num_outputs))\n",
    "    \n",
    "    # F_W = tf.Variable(tf.random_normal([input_size, num_outputs]))\n",
    "    # F_b = tf.Variable(tf.random_normal([num_outputs]))\n",
    "    \n",
    "    shape_W = [input_size, num_outputs]\n",
    "    shape_b = [num_outputs]\n",
    "    F_W = tf.Variable(xavier_init(shape_W, input_size, num_outputs))\n",
    "    F_b = tf.Variable(xavier_init(shape_b, input_size, num_outputs))\n",
    "    \n",
    "    x_tensor = tf.add(tf.matmul(x_tensor, F_W), F_b)\n",
    "    x_tensor = tf.nn.relu(x_tensor)\n",
    "    return x_tensor\n",
    "\n",
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_size = x_tensor.get_shape().as_list()[1]\n",
    "    print(\"Add output: input[%d] output[%d]\" % (input_size,num_outputs))\n",
    "    F_W = tf.Variable(tf.random_normal([input_size, num_outputs]))\n",
    "    F_b = tf.Variable(tf.random_normal([num_outputs]))\n",
    "    x_tensor = tf.add(tf.matmul(x_tensor, F_W), F_b)\n",
    "    return x_tensor\n",
    "\n",
    "# Define conv-max-pool layer parameters\n",
    "convParams1 = pd.DataFrame({\n",
    "    'layer':range(4),\n",
    "    'conv_kernal':[5]*4,\n",
    "    'conv_stride':[1]*4,\n",
    "    'pool_kernal':[2]*4,\n",
    "    'pool_stride':[2]*4,\n",
    "    'num_output':[16*(2**number) for number in range(0,4)]\n",
    "})\n",
    "\n",
    "convParams2 = pd.DataFrame({\n",
    "    'layer':range(4),\n",
    "    'conv_kernal':[5]*4,\n",
    "    'conv_stride':[1]*4,\n",
    "    'pool_kernal':[2]*4,\n",
    "    'pool_stride':[2]*4,\n",
    "    'num_output':[32*(2**number) for number in range(0,4)]\n",
    "})\n",
    "\n",
    "convParams3 = pd.DataFrame({\n",
    "    'layer':range(4),\n",
    "    'conv_kernal':[4]*4,\n",
    "    'conv_stride':[2]*4,\n",
    "    'pool_kernal':[2]*4,\n",
    "    'pool_stride':[2]*4,\n",
    "    'num_output':[32*(4**number) for number in range(0,4)]\n",
    "})\n",
    "\n",
    "convParamsList = [convParams1,convParams2,convParams3]\n",
    "\n",
    "fullParams = [[1024],[512],[512, 128], [1024, 128],[1024, 256, 64]]\n",
    "\n",
    "\n",
    "def conv_net(x, keep_prob, conv_plan=1, conv_depth=1, full_plan=1, dropout_fc=True, dropout_conv=False, relu=True):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    print(\"Build conv_net conv_plan[%d] conv_depth[%d] full_plan[%d]\" % (conv_plan,conv_depth,full_plan))\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    convParams = convParamsList[conv_plan-1]\n",
    "    for i in range(conv_depth):\n",
    "        # def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides, keep_prob=None, dropout=False, relu=True):\n",
    "        conv_num_outputs = convParams.at[i,'num_output']\n",
    "        conv_ksize = [convParams.at[i,'conv_kernal'],convParams.at[i,'conv_kernal']]\n",
    "        conv_strides = [convParams.at[i,'conv_stride'],convParams.at[i,'conv_stride']]\n",
    "        pool_ksize = [convParams.at[i,'pool_kernal'],convParams.at[i,'pool_kernal']]\n",
    "        pool_strides = [convParams.at[i,'pool_stride'],convParams.at[i,'pool_stride']]\n",
    "        x = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides, keep_prob, dropout_conv, relu)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    x = flatten(x)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    full_params = fullParams[full_plan-1]\n",
    "    if dropout_fc:\n",
    "        x = fully_conn(x, full_params[0])\n",
    "        x = tf.nn.dropout(x, keep_prob)\n",
    "        print(\"Add dropout layer\")\n",
    "        for i in range(1,len(full_params)):\n",
    "            x = fully_conn(x, full_params[i])\n",
    "    else:\n",
    "        for i in range(len(full_params)):\n",
    "             x = fully_conn(x, full_params[i])\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    n_classes = 10\n",
    "    x = output(x, n_classes)\n",
    "    \n",
    "    # TODO: return output\n",
    "    print(\"\")\n",
    "    return x\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "# logits = conv_net(x, keep_prob, conv_plan=2, conv_depth=4, full_plan=4, dropout_fc=True, dropout_conv=False, relu=True)\n",
    "# logits = conv_net(x, keep_prob, conv_plan=3, conv_depth=2, full_plan=4, dropout_fc=True, dropout_conv=True, relu=True)\n",
    "# logits = conv_net(x, keep_prob, conv_plan=2, conv_depth=3, full_plan=4, dropout_fc=True, dropout_conv=False, relu=False)\n",
    "logits = conv_net(x, keep_prob, conv_plan=3, conv_depth=3, full_plan=5, dropout_fc=True, dropout_conv=True, relu=True)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs = 100\n",
      "batch_size = 64\n",
      "keep_probability = 0.75\n",
      "Checking the Training on a Single Batch...\n",
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.1929 Validation Accuracy: 0.232600\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     1.9065 Validation Accuracy: 0.358800\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     1.4687 Validation Accuracy: 0.365000\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.6329 Validation Accuracy: 0.422000\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.7648 Validation Accuracy: 0.391600\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.8185 Validation Accuracy: 0.411800\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.6295 Validation Accuracy: 0.457400\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.2104 Validation Accuracy: 0.449200\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.3570 Validation Accuracy: 0.482000\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.4765 Validation Accuracy: 0.490200\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.4684 Validation Accuracy: 0.479200\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.4286 Validation Accuracy: 0.509800\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.1465 Validation Accuracy: 0.496400\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.3132 Validation Accuracy: 0.523000\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.4361 Validation Accuracy: 0.509800\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.3438 Validation Accuracy: 0.515800\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.2974 Validation Accuracy: 0.535600\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.0127 Validation Accuracy: 0.522400\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.1623 Validation Accuracy: 0.546000\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.3152 Validation Accuracy: 0.537000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.2234 Validation Accuracy: 0.545800\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.2593 Validation Accuracy: 0.542400\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     0.9416 Validation Accuracy: 0.533800\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.0996 Validation Accuracy: 0.550400\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.2786 Validation Accuracy: 0.554800\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.1642 Validation Accuracy: 0.557400\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.1351 Validation Accuracy: 0.567800\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     0.8736 Validation Accuracy: 0.567400\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.0950 Validation Accuracy: 0.580600\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.1915 Validation Accuracy: 0.558400\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.1341 Validation Accuracy: 0.555800\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.0734 Validation Accuracy: 0.570200\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     0.8039 Validation Accuracy: 0.580000\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.0168 Validation Accuracy: 0.577600\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.2552 Validation Accuracy: 0.562000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.0842 Validation Accuracy: 0.581200\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.0980 Validation Accuracy: 0.592200\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     0.7967 Validation Accuracy: 0.584800\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     0.9575 Validation Accuracy: 0.587400\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.1593 Validation Accuracy: 0.575400\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.0450 Validation Accuracy: 0.586200\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     0.9862 Validation Accuracy: 0.593000\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     0.7459 Validation Accuracy: 0.603400\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     0.9403 Validation Accuracy: 0.589400\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     0.9720 Validation Accuracy: 0.604600\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.0255 Validation Accuracy: 0.594600\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.1249 Validation Accuracy: 0.592600\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     0.6636 Validation Accuracy: 0.602000\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     0.8654 Validation Accuracy: 0.597400\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.0293 Validation Accuracy: 0.594800\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.0699 Validation Accuracy: 0.591600\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     0.9785 Validation Accuracy: 0.598800\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     0.6984 Validation Accuracy: 0.615400\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     0.8556 Validation Accuracy: 0.589600\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     0.8809 Validation Accuracy: 0.605200\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     0.9715 Validation Accuracy: 0.604200\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     1.0697 Validation Accuracy: 0.612000\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     0.6805 Validation Accuracy: 0.600400\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     0.8263 Validation Accuracy: 0.602600\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     0.8186 Validation Accuracy: 0.606600\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.0297 Validation Accuracy: 0.583800\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.0257 Validation Accuracy: 0.614800\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     0.6261 Validation Accuracy: 0.620000\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     0.8081 Validation Accuracy: 0.612000\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     0.8239 Validation Accuracy: 0.602000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     0.9511 Validation Accuracy: 0.602600\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     0.9790 Validation Accuracy: 0.604600\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     0.6142 Validation Accuracy: 0.612800\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     0.7669 Validation Accuracy: 0.613000\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     0.8253 Validation Accuracy: 0.616000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     0.9810 Validation Accuracy: 0.596000\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     0.9421 Validation Accuracy: 0.607000\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     0.5198 Validation Accuracy: 0.621200\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     0.7720 Validation Accuracy: 0.607200\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     0.7772 Validation Accuracy: 0.616600\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     0.9350 Validation Accuracy: 0.610400\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     0.8742 Validation Accuracy: 0.620200\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     0.5516 Validation Accuracy: 0.637000\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     0.6799 Validation Accuracy: 0.621000\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     0.7537 Validation Accuracy: 0.625200\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     0.9004 Validation Accuracy: 0.628000\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     0.8263 Validation Accuracy: 0.624400\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     0.5655 Validation Accuracy: 0.634800\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     0.6902 Validation Accuracy: 0.621600\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     0.8909 Validation Accuracy: 0.598600\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     0.8321 Validation Accuracy: 0.618200\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     0.8526 Validation Accuracy: 0.619600\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     0.4713 Validation Accuracy: 0.631000\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     0.6884 Validation Accuracy: 0.632800\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     0.7313 Validation Accuracy: 0.630600\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     0.8695 Validation Accuracy: 0.624400\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     0.7697 Validation Accuracy: 0.624200\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     0.4822 Validation Accuracy: 0.635200\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     0.6983 Validation Accuracy: 0.635200\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     0.6895 Validation Accuracy: 0.614200\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     0.7776 Validation Accuracy: 0.636400\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     0.7818 Validation Accuracy: 0.626000\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     0.4892 Validation Accuracy: 0.632400\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     0.6901 Validation Accuracy: 0.637200\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     0.6079 Validation Accuracy: 0.630400\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     0.8325 Validation Accuracy: 0.619200\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     0.8745 Validation Accuracy: 0.624800\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     0.4823 Validation Accuracy: 0.644000\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     0.6674 Validation Accuracy: 0.628400\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     0.6741 Validation Accuracy: 0.621800\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     0.7899 Validation Accuracy: 0.626200\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     0.8218 Validation Accuracy: 0.617600\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     0.4767 Validation Accuracy: 0.634400\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     0.5984 Validation Accuracy: 0.624800\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     0.6430 Validation Accuracy: 0.625400\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.8404 Validation Accuracy: 0.621000\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     0.7509 Validation Accuracy: 0.638400\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.4329 Validation Accuracy: 0.646600\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     0.6196 Validation Accuracy: 0.626400\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     0.5453 Validation Accuracy: 0.630600\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.8127 Validation Accuracy: 0.638600\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     0.8171 Validation Accuracy: 0.608000\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.4482 Validation Accuracy: 0.642800\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     0.5703 Validation Accuracy: 0.641000\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     0.5882 Validation Accuracy: 0.634400\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.7540 Validation Accuracy: 0.635200\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     0.6931 Validation Accuracy: 0.640400\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.4034 Validation Accuracy: 0.629400\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     0.6332 Validation Accuracy: 0.640800\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     0.5647 Validation Accuracy: 0.636000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.7466 Validation Accuracy: 0.629400\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     0.6353 Validation Accuracy: 0.638800\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     0.3753 Validation Accuracy: 0.644200\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     0.5929 Validation Accuracy: 0.635000\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     0.5582 Validation Accuracy: 0.645000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.7544 Validation Accuracy: 0.632200\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     0.6496 Validation Accuracy: 0.637000\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     0.3729 Validation Accuracy: 0.654400\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     0.5807 Validation Accuracy: 0.624400\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     0.5185 Validation Accuracy: 0.637200\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.6845 Validation Accuracy: 0.644000\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     0.6882 Validation Accuracy: 0.630600\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     0.4163 Validation Accuracy: 0.648000\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     0.5939 Validation Accuracy: 0.624600\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     0.5867 Validation Accuracy: 0.626400\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.6349 Validation Accuracy: 0.644400\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     0.6209 Validation Accuracy: 0.649000\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     0.3963 Validation Accuracy: 0.650000\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     0.5295 Validation Accuracy: 0.646000\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     0.4855 Validation Accuracy: 0.626800\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.6549 Validation Accuracy: 0.634200\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     0.5997 Validation Accuracy: 0.646400\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.3658 Validation Accuracy: 0.649400\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     0.5624 Validation Accuracy: 0.637800\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     0.5813 Validation Accuracy: 0.628800\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.6286 Validation Accuracy: 0.640000\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     0.5619 Validation Accuracy: 0.643000\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     0.4283 Validation Accuracy: 0.649000\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     0.5359 Validation Accuracy: 0.638200\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     0.5002 Validation Accuracy: 0.633600\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.6105 Validation Accuracy: 0.643000\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     0.6399 Validation Accuracy: 0.641000\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.4529 Validation Accuracy: 0.633200\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     0.5553 Validation Accuracy: 0.630200\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     0.4788 Validation Accuracy: 0.639800\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.5626 Validation Accuracy: 0.640800\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     0.6199 Validation Accuracy: 0.648400\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.3478 Validation Accuracy: 0.646400\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     0.4954 Validation Accuracy: 0.635800\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     0.4621 Validation Accuracy: 0.643200\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.6405 Validation Accuracy: 0.644200\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     0.5775 Validation Accuracy: 0.658200\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.3944 Validation Accuracy: 0.649400\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     0.5815 Validation Accuracy: 0.655000\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     0.5295 Validation Accuracy: 0.627600\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.5776 Validation Accuracy: 0.641600\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     0.5963 Validation Accuracy: 0.641400\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.4466 Validation Accuracy: 0.636800\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     0.5536 Validation Accuracy: 0.645400\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     0.4321 Validation Accuracy: 0.642200\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.6365 Validation Accuracy: 0.644600\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     0.5586 Validation Accuracy: 0.649600\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     0.3738 Validation Accuracy: 0.646400\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     0.4669 Validation Accuracy: 0.658200\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     0.4462 Validation Accuracy: 0.658800\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.5538 Validation Accuracy: 0.643800\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     0.5462 Validation Accuracy: 0.650000\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.3215 Validation Accuracy: 0.636400\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     0.5007 Validation Accuracy: 0.646200\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     0.4891 Validation Accuracy: 0.629200\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.6392 Validation Accuracy: 0.643200\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     0.5439 Validation Accuracy: 0.656200\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.3388 Validation Accuracy: 0.652400\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     0.4626 Validation Accuracy: 0.644000\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     0.4567 Validation Accuracy: 0.653200\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.6087 Validation Accuracy: 0.640000\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     0.5053 Validation Accuracy: 0.650400\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.3567 Validation Accuracy: 0.643800\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     0.4865 Validation Accuracy: 0.645600\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     0.3489 Validation Accuracy: 0.642600\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.6348 Validation Accuracy: 0.633600\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.5115 Validation Accuracy: 0.644000\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.3296 Validation Accuracy: 0.663800\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     0.5584 Validation Accuracy: 0.656800\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     0.4141 Validation Accuracy: 0.641600\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.5907 Validation Accuracy: 0.637200\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.5949 Validation Accuracy: 0.643600\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.3460 Validation Accuracy: 0.649200\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     0.6004 Validation Accuracy: 0.642600\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     0.4141 Validation Accuracy: 0.652000\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.5053 Validation Accuracy: 0.652800\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.4611 Validation Accuracy: 0.660600\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.3262 Validation Accuracy: 0.657600\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     0.4905 Validation Accuracy: 0.659600\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.4062 Validation Accuracy: 0.648400\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.5815 Validation Accuracy: 0.640200\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.5225 Validation Accuracy: 0.658600\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.3750 Validation Accuracy: 0.658400\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     0.4542 Validation Accuracy: 0.656200\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.3721 Validation Accuracy: 0.649200\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.5828 Validation Accuracy: 0.637200\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.4815 Validation Accuracy: 0.652800\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.3266 Validation Accuracy: 0.661800\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     0.4927 Validation Accuracy: 0.652600\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.4268 Validation Accuracy: 0.625400\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.4856 Validation Accuracy: 0.652600\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.5080 Validation Accuracy: 0.660200\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.3184 Validation Accuracy: 0.653400\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.5284 Validation Accuracy: 0.649600\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.4159 Validation Accuracy: 0.639800\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.4650 Validation Accuracy: 0.658600\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.4482 Validation Accuracy: 0.654200\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.3159 Validation Accuracy: 0.656000\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     0.4453 Validation Accuracy: 0.652600\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.3549 Validation Accuracy: 0.646000\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.4658 Validation Accuracy: 0.658000\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.5321 Validation Accuracy: 0.645400\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.2586 Validation Accuracy: 0.669400\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.5019 Validation Accuracy: 0.650400\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.3559 Validation Accuracy: 0.650200\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.4476 Validation Accuracy: 0.659800\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.4304 Validation Accuracy: 0.655600\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.2745 Validation Accuracy: 0.656800\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.3828 Validation Accuracy: 0.652400\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.3460 Validation Accuracy: 0.657200\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.4818 Validation Accuracy: 0.655000\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.4590 Validation Accuracy: 0.653800\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.2648 Validation Accuracy: 0.654400\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     0.4298 Validation Accuracy: 0.653200\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.3141 Validation Accuracy: 0.652400\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.5075 Validation Accuracy: 0.645000\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.4778 Validation Accuracy: 0.657600\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.2912 Validation Accuracy: 0.661400\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     0.3878 Validation Accuracy: 0.661400\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.3413 Validation Accuracy: 0.650200\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.4272 Validation Accuracy: 0.634800\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.4771 Validation Accuracy: 0.643600\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.2387 Validation Accuracy: 0.660200\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.4373 Validation Accuracy: 0.650200\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.3404 Validation Accuracy: 0.638800\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.4852 Validation Accuracy: 0.647600\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.4945 Validation Accuracy: 0.646200\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.2621 Validation Accuracy: 0.646800\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     0.4807 Validation Accuracy: 0.642600\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.3331 Validation Accuracy: 0.663600\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.5640 Validation Accuracy: 0.643400\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.4711 Validation Accuracy: 0.653000\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.2538 Validation Accuracy: 0.650000\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.4901 Validation Accuracy: 0.635800\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.3030 Validation Accuracy: 0.651000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.4747 Validation Accuracy: 0.658800\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.4945 Validation Accuracy: 0.672400\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.2070 Validation Accuracy: 0.663200\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.4372 Validation Accuracy: 0.661200\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.3053 Validation Accuracy: 0.658200\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.4379 Validation Accuracy: 0.661400\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.3923 Validation Accuracy: 0.654600\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.2525 Validation Accuracy: 0.650000\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.3968 Validation Accuracy: 0.653000\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.2996 Validation Accuracy: 0.656400\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.4062 Validation Accuracy: 0.653600\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.3942 Validation Accuracy: 0.665800\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.2837 Validation Accuracy: 0.665200\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.3968 Validation Accuracy: 0.657600\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.4004 Validation Accuracy: 0.642600\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.4137 Validation Accuracy: 0.653600\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.3518 Validation Accuracy: 0.657400\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.2096 Validation Accuracy: 0.666200\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.4386 Validation Accuracy: 0.669000\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.3284 Validation Accuracy: 0.661200\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.4821 Validation Accuracy: 0.658800\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.4003 Validation Accuracy: 0.661800\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.2418 Validation Accuracy: 0.665000\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.4227 Validation Accuracy: 0.663800\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.3589 Validation Accuracy: 0.646600\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.4588 Validation Accuracy: 0.655400\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.3669 Validation Accuracy: 0.659600\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.2564 Validation Accuracy: 0.663200\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.4309 Validation Accuracy: 0.659600\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.2963 Validation Accuracy: 0.668200\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.3669 Validation Accuracy: 0.655800\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.3630 Validation Accuracy: 0.650000\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.2813 Validation Accuracy: 0.655200\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.4456 Validation Accuracy: 0.659200\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.3416 Validation Accuracy: 0.652000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.4332 Validation Accuracy: 0.657400\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.4167 Validation Accuracy: 0.656200\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.2543 Validation Accuracy: 0.658000\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.3931 Validation Accuracy: 0.652800\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.3084 Validation Accuracy: 0.658800\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.3752 Validation Accuracy: 0.654200\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.4621 Validation Accuracy: 0.642000\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.2465 Validation Accuracy: 0.660600\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.3970 Validation Accuracy: 0.661600\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.3178 Validation Accuracy: 0.645600\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.3513 Validation Accuracy: 0.661000\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.3108 Validation Accuracy: 0.659400\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.2333 Validation Accuracy: 0.666200\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.3717 Validation Accuracy: 0.658000\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.2672 Validation Accuracy: 0.655200\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.3763 Validation Accuracy: 0.653600\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.4185 Validation Accuracy: 0.649800\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.2500 Validation Accuracy: 0.670400\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.3826 Validation Accuracy: 0.664800\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.3546 Validation Accuracy: 0.651800\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.3434 Validation Accuracy: 0.666200\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.4596 Validation Accuracy: 0.666800\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.2216 Validation Accuracy: 0.671200\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.3883 Validation Accuracy: 0.660600\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.4055 Validation Accuracy: 0.635400\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.4120 Validation Accuracy: 0.648400\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.3440 Validation Accuracy: 0.666000\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.2766 Validation Accuracy: 0.663400\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.3353 Validation Accuracy: 0.664600\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.3699 Validation Accuracy: 0.648000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.3188 Validation Accuracy: 0.658800\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.3903 Validation Accuracy: 0.667400\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.2281 Validation Accuracy: 0.662000\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.3789 Validation Accuracy: 0.670400\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.3358 Validation Accuracy: 0.640800\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.3610 Validation Accuracy: 0.659200\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.4156 Validation Accuracy: 0.658200\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.1978 Validation Accuracy: 0.668400\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.3914 Validation Accuracy: 0.661800\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.3045 Validation Accuracy: 0.660200\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.4034 Validation Accuracy: 0.656600\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.2885 Validation Accuracy: 0.651200\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.2166 Validation Accuracy: 0.657800\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.3649 Validation Accuracy: 0.665000\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.3272 Validation Accuracy: 0.649200\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.3311 Validation Accuracy: 0.660600\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.3232 Validation Accuracy: 0.653200\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.2194 Validation Accuracy: 0.666800\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.3683 Validation Accuracy: 0.661200\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.3539 Validation Accuracy: 0.651800\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.3168 Validation Accuracy: 0.658000\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.3130 Validation Accuracy: 0.656600\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.2468 Validation Accuracy: 0.658000\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.3488 Validation Accuracy: 0.654800\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.3834 Validation Accuracy: 0.655800\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.3381 Validation Accuracy: 0.663800\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.2498 Validation Accuracy: 0.667600\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.1897 Validation Accuracy: 0.668000\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.3524 Validation Accuracy: 0.670200\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.3301 Validation Accuracy: 0.661200\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.3579 Validation Accuracy: 0.657400\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.3036 Validation Accuracy: 0.659800\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.2381 Validation Accuracy: 0.660200\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.3405 Validation Accuracy: 0.669400\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.3152 Validation Accuracy: 0.671600\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.3549 Validation Accuracy: 0.675400\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.2431 Validation Accuracy: 0.671400\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.2066 Validation Accuracy: 0.662200\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.3275 Validation Accuracy: 0.666400\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.2806 Validation Accuracy: 0.652400\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.3241 Validation Accuracy: 0.666800\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.2669 Validation Accuracy: 0.670000\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.2190 Validation Accuracy: 0.655000\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.3413 Validation Accuracy: 0.654600\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.2764 Validation Accuracy: 0.657800\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.2949 Validation Accuracy: 0.662400\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.3492 Validation Accuracy: 0.647400\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.2070 Validation Accuracy: 0.658600\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.3094 Validation Accuracy: 0.668800\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.2649 Validation Accuracy: 0.644600\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.3308 Validation Accuracy: 0.662000\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.3818 Validation Accuracy: 0.655000\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.2009 Validation Accuracy: 0.661400\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.3070 Validation Accuracy: 0.660000\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.2956 Validation Accuracy: 0.652400\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.3392 Validation Accuracy: 0.656800\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.2647 Validation Accuracy: 0.670400\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.2173 Validation Accuracy: 0.666200\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.3493 Validation Accuracy: 0.657400\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.2987 Validation Accuracy: 0.657600\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.3129 Validation Accuracy: 0.667000\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.3564 Validation Accuracy: 0.649200\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.1871 Validation Accuracy: 0.669200\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.4320 Validation Accuracy: 0.661800\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.2633 Validation Accuracy: 0.661200\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.3210 Validation Accuracy: 0.663800\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.3918 Validation Accuracy: 0.659200\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.2334 Validation Accuracy: 0.663200\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.3620 Validation Accuracy: 0.670800\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.3146 Validation Accuracy: 0.643400\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.2769 Validation Accuracy: 0.671600\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.3340 Validation Accuracy: 0.665800\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.2627 Validation Accuracy: 0.659600\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.3392 Validation Accuracy: 0.666800\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.2175 Validation Accuracy: 0.654800\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.2638 Validation Accuracy: 0.657400\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.3045 Validation Accuracy: 0.664400\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.2076 Validation Accuracy: 0.672000\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.3441 Validation Accuracy: 0.670200\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.2596 Validation Accuracy: 0.667200\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.3934 Validation Accuracy: 0.658400\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.3427 Validation Accuracy: 0.650000\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.2107 Validation Accuracy: 0.663000\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.3043 Validation Accuracy: 0.667400\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.2096 Validation Accuracy: 0.654000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.2901 Validation Accuracy: 0.663200\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.4063 Validation Accuracy: 0.653200\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.2060 Validation Accuracy: 0.670800\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.3511 Validation Accuracy: 0.667400\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.2332 Validation Accuracy: 0.653800\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.3073 Validation Accuracy: 0.662000\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.2869 Validation Accuracy: 0.657000\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.1900 Validation Accuracy: 0.659200\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.3032 Validation Accuracy: 0.659000\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.2028 Validation Accuracy: 0.663600\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.3148 Validation Accuracy: 0.659400\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.2573 Validation Accuracy: 0.667600\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.2171 Validation Accuracy: 0.667400\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.3392 Validation Accuracy: 0.664200\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.1962 Validation Accuracy: 0.668200\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.2893 Validation Accuracy: 0.664800\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.2691 Validation Accuracy: 0.662800\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.2083 Validation Accuracy: 0.661200\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.3359 Validation Accuracy: 0.662400\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.2360 Validation Accuracy: 0.655000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.3219 Validation Accuracy: 0.657200\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.4067 Validation Accuracy: 0.654600\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.1973 Validation Accuracy: 0.663800\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.2643 Validation Accuracy: 0.675000\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.2957 Validation Accuracy: 0.660600\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.2833 Validation Accuracy: 0.671800\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.2412 Validation Accuracy: 0.665600\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.2184 Validation Accuracy: 0.661200\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.2855 Validation Accuracy: 0.659400\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.2016 Validation Accuracy: 0.664200\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.2785 Validation Accuracy: 0.664600\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.3154 Validation Accuracy: 0.668400\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.2281 Validation Accuracy: 0.673800\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.2849 Validation Accuracy: 0.670600\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.2248 Validation Accuracy: 0.669000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.2413 Validation Accuracy: 0.649800\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.2759 Validation Accuracy: 0.662800\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.1939 Validation Accuracy: 0.675200\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.2526 Validation Accuracy: 0.674600\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.2225 Validation Accuracy: 0.673600\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.2639 Validation Accuracy: 0.662400\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.2846 Validation Accuracy: 0.661400\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.1814 Validation Accuracy: 0.662800\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.3344 Validation Accuracy: 0.661400\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.2481 Validation Accuracy: 0.660200\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.2862 Validation Accuracy: 0.661800\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.2748 Validation Accuracy: 0.654600\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.1747 Validation Accuracy: 0.673800\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.2773 Validation Accuracy: 0.668800\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.2045 Validation Accuracy: 0.666200\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.2880 Validation Accuracy: 0.662200\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.2595 Validation Accuracy: 0.660800\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.1888 Validation Accuracy: 0.666600\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.2751 Validation Accuracy: 0.661600\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.2290 Validation Accuracy: 0.669400\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.2802 Validation Accuracy: 0.667400\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.2829 Validation Accuracy: 0.676800\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.1520 Validation Accuracy: 0.675400\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.2937 Validation Accuracy: 0.662200\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.2933 Validation Accuracy: 0.650200\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.2536 Validation Accuracy: 0.662600\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.2868 Validation Accuracy: 0.663200\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.2147 Validation Accuracy: 0.662000\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.3145 Validation Accuracy: 0.660200\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.2087 Validation Accuracy: 0.665800\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.2474 Validation Accuracy: 0.672200\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.2901 Validation Accuracy: 0.657000\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.1826 Validation Accuracy: 0.671800\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.2598 Validation Accuracy: 0.670000\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.2397 Validation Accuracy: 0.664400\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.2508 Validation Accuracy: 0.665400\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.3510 Validation Accuracy: 0.660600\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.1758 Validation Accuracy: 0.662600\n",
      "Epoch 98, CIFAR-10 Batch 4:  "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "keep_probability = 0.75\n",
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # Calculate batch loss and accuracy\n",
    "    loss = session.run(cost, feed_dict={'x:0': feature_batch, 'y:0': label_batch, 'keep_prob:0': 1.})\n",
    "    valid_acc = session.run(accuracy, feed_dict={'x:0': valid_features, 'y:0': valid_labels, 'keep_prob:0': 1.})\n",
    "\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss,valid_acc))\n",
    "\n",
    "print(\"epochs = %d\" % epochs)\n",
    "print(\"batch_size = %d\" % batch_size)\n",
    "print(\"keep_probability = %.2f\" % keep_probability)\n",
    "print('Checking the Training on a Single Batch...')\n",
    "start_time = time.time()\n",
    "\n",
    "save_model_path = './image_class_model_orig'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
